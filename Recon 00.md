## Recon 00
This exercise covers the robots.txt file

### OBJECTIVE
For this challenge, your goal is to retrieve the robots.txt from the main website for hackycorp.com.

### THE ROBOTS.TXT FILE
The robots.txt file is used to tell web spiders how to crawl a website. 
To avoid having confidential information indexed and searchable, webmasters often use this file to tell spiders to avoid specific pages. 
This is done using the keyword **Disallow**. You can find more about the robots.txt file by reading [Robots exclusion standard](https://en.wikipedia.org/wiki/Robots_exclusion_standard)

## Solution
- Add /robots.txt to the end of the given url - the **key** will appear<br>
![image](https://user-images.githubusercontent.com/73820496/187874261-6714f17d-5bd6-424e-b83d-cc4ef03d0d7a.png)
