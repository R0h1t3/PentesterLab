# Introduction 01
This exercise will guide through the process of scoring an exercise to mark it as completed

### THE ROBOTS.TXT FILE
The **robots.txt** file is used to tell web spiders how to crawl a website.

To avoid having confidential information indexed and searchable, webmasters often use this file to tell spiders to avoid specific pages. This is done using the keyword Disallow.

As an attacker, it's always a good idea to check the content of the robots.txt file, and visit all the pages that are "disallowed", in order to find sensitive information.

## Solution
Going the **/robots.txt** file in the given domain will show us the file's content which are the URLs that an engine is allowed and disallowed.<br>
Browse through them to find the flag.<br>

![image](https://user-images.githubusercontent.com/73820496/190370767-4cf7c02f-4a09-4b1a-bc80-51d4331cd896.png)

